{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brainapp12/apps/blob/main/Talentum_TPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6HFkgL5v0oq"
      },
      "source": [
        "#5.Streamlit\n",
        "- import のところ以前のを整理^^;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v-h0nfP_kOmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096a2dc7-d47c-46cb-81b4-e303f8e9bb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "#%%writefile app.py\n",
        "import os\n",
        "import tqdm\n",
        "import urllib\n",
        "import io\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import itertools\n",
        "import requests\n",
        "import json\n",
        "import datetime\n",
        "from dateutil import relativedelta\n",
        "from datetime import date\n",
        "import collections\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def request_search(bearer_token, params, max_count):\n",
        "\n",
        "    tweets = []\n",
        "    expanded = {\n",
        "        \"tweets\": [],\n",
        "        \"users\": []\n",
        "    }\n",
        "\n",
        "    next_token = None\n",
        "\n",
        "    while True:\n",
        "        if next_token is not None:\n",
        "            params[\"next_token\"] = next_token\n",
        "\n",
        "        url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "        encoded_params = urllib.parse.urlencode(params)\n",
        "        headers = {\"Authorization\": f\"Bearer AAAAAAAAAAAAAAAAAAAAAJ5bZwEAAAAAdlSGg9zHc8pH2IKr5HgBllUg3SA%3D3ZpLpfDZHf4Sd01v4np7rJcr3DyEXBfNTWBfpOGST30OtqGGdG\"}\n",
        "        res = requests.request(\n",
        "            \"GET\", url, params=encoded_params, headers=headers)\n",
        "\n",
        "        if res.status_code == 429:\n",
        "            rate_limit_reset = int(res.headers[\"x-rate-limit-reset\"])\n",
        "            now = time.mktime(datetime.datetime.now().timetuple())\n",
        "            wait_sec = int(rate_limit_reset - now)\n",
        "            desc = f\"Waiting for {wait_sec} seconds\"\n",
        "            for _ in tqdm.trange(wait_sec, desc=desc):\n",
        "                time.sleep(1)\n",
        "\n",
        "        elif res.status_code != 200:\n",
        "            raise Exception(res.status_code, res.text)\n",
        "\n",
        "        else:\n",
        "            res_json = res.json()\n",
        "\n",
        "            if res_json[\"meta\"][\"result_count\"] == 0:\n",
        "                break\n",
        "\n",
        "            tweets += res_json[\"data\"]\n",
        "            print(f\"{len(tweets)}件のツイートを取得しました。\")\n",
        "\n",
        "            if res_json.get(\"includes\"):\n",
        "                includes = res_json[\"includes\"]\n",
        "                for k, v in expanded.items():\n",
        "                    if includes.get(k):\n",
        "                        \n",
        "                        expanded[k] += includes[k]\n",
        "\n",
        "            next_token = res_json.get(\"meta\").get(\"next_token\")\n",
        "            if next_token is None or len(tweets) >= max_count:\n",
        "                break\n",
        "\n",
        "    return tweets[:max_count], expanded\n",
        "\n",
        "\n",
        "def export_json(fpath, data):\n",
        "    with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def search_tweet(max_count,keyword,value):\n",
        "\n",
        "  dt_now_jst_aware = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  print((dt_now_jst_aware - relativedelta.relativedelta(hours=value)).isoformat())\n",
        "  print((dt_now_jst_aware - relativedelta.relativedelta(minutes=1)).isoformat())\n",
        "\n",
        "\n",
        "  bearer_token = \"AAAAAAAAAAAAAAAAAAAAAJ5bZwEAAAAAdlSGg9zHc8pH2IKr5HgBllUg3SA%3D3ZpLpfDZHf4Sd01v4np7rJcr3DyEXBfNTWBfpOGST30OtqGGdG\"\n",
        "  max_count = max_count\n",
        "\n",
        "  params = {\n",
        "      \"query\":keyword,\n",
        "      \"start_time\": (dt_now_jst_aware - relativedelta.relativedelta(hours=value)).isoformat(),\n",
        "      \"end_time\": (dt_now_jst_aware - relativedelta.relativedelta(minutes=1)).isoformat(),\n",
        "      \"expansions\": \"author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id\",\n",
        "      \"max_results\": \"100\",\n",
        "      \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics\",\n",
        "      \"place.fields\": \"contained_within,country,country_code,full_name,geo,id,name,place_type\",\n",
        "      \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\",\n",
        "      \"tweet.fields\": \"attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\",\n",
        "      \"user.fields\": \"created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld\"\n",
        "  }\n",
        "\n",
        "\n",
        "  tweets, expanded = request_search(bearer_token, params, max_count)\n",
        "\n",
        "\n",
        "  df_tweet = pd.DataFrame()\n",
        "  df_tweet_ex = pd.DataFrame()\n",
        "  for i in range(0,len(tweets)):\n",
        "      try:\n",
        "          df_tweet.loc[i,'author_id'] = tweets[i]['author_id']\n",
        "          df_tweet.loc[i,'text'] = tweets[i]['text']\n",
        "          df_tweet.loc[i,'like_count'] = tweets[i]['public_metrics']['like_count']\n",
        "          df_tweet.loc[i,'retweet_count'] = tweets[i]['public_metrics']['retweet_count']\n",
        "          df_tweet_ex.loc[i,'name'] = expanded['users'][i]['name']\n",
        "          df_tweet_ex.loc[i,'username'] = expanded['users'][i]['username']\n",
        "          df_tweet_ex.loc[i,'description'] = expanded['users'][i]['description']\n",
        "          df_tweet_ex.loc[i,'author_id'] = expanded['users'][i]['id']\n",
        "\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "\n",
        "  df_tweet = df_tweet.merge(df_tweet_ex,on='author_id',how='inner')\n",
        "  df_tweet['url'] = df_tweet['username'].apply(lambda x: 'https://twitter.com/' + str(x))\n",
        "  df_tweet = df_tweet[['name','username','text','description','retweet_count','like_count','url']]\n",
        "  return df_tweet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import base64\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"Talentum：Talent Pool Automation\")\n",
        "cnt=st.number_input('探索ツイート数の設定：0~50000',0,50000,0,step=1)\n",
        "keyword = st.text_input('人材探索キーワードの設定 半角で入力ください')\n",
        "st.text_area('分析メモ')\n",
        "\n",
        "\n",
        "talent_search = st.button(\"Search Talent\")\n",
        "if talent_search :\n",
        "  df_tweet = search_tweet(cnt,keyword,24*6.9)\n",
        "  df_talent = df_tweet[['username','text','description','url']].rename(columns={'username':'ユーザーID','text':'ツイート本文','description':'プロフィール','url':'ツイートのURL'})\n",
        "  df_talent = df_talent.groupby('ツイート本文',as_index=False).head(1)\n",
        "df_talent\n",
        "\n",
        "\n",
        "csv = df_talent.to_csv(index=False)  \n",
        "b64 = base64.b64encode(csv.encode()).decode()\n",
        "href = f'<a href=\"data:application/octet-stream;base64,{b64}\" download=\"result_utf-8.csv\">Download Link</a>'\n",
        "st.markdown(f\"人材探索データのダウンロード（csv）:  {href}\", unsafe_allow_html=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}